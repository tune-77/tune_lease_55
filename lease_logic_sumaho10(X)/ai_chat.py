"""
AI ãƒãƒ£ãƒƒãƒˆãƒ»ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆé–¢é€£ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–¢æ•°ã€‚
Ollama / Gemini API ã®å‘¼ã³å‡ºã—ãƒ»ãƒªãƒˆãƒ©ã‚¤ãƒ»æ¥ç¶šãƒ†ã‚¹ãƒˆã‚’ã¾ã¨ã‚ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
UIï¼ˆStreamlit å´ï¼‰ã¯ lease_logic_sumaho10.py ã«æ®‹ã™ã€‚
"""
import os
import json
import time
import concurrent.futures
import datetime
import streamlit as st

from config import (
    OLLAMA_MODEL,
    GEMINI_API_KEY_ENV,
    GEMINI_MODEL_DEFAULT,
    DEBATE_FILE,
)

# ã‚¹ãƒ¬ãƒƒãƒ‰ â†’ ãƒ¡ã‚¤ãƒ³ã§çµæœã‚’æ¸¡ã™ç”¨ï¼ˆsession_state ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰æ›´æ–°ä¸å¯ï¼‰
_chat_result_holder: dict = {"result": None, "done": False}


def _get_gemini_key_from_secrets() -> str:
    """secrets.toml ãŒç„¡ãã¦ã‚‚ä¾‹å¤–ã«ã—ãªã„ã€‚ã‚­ãƒ¼ãŒã‚ã‚Œã°è¿”ã™ã€‚"""
    try:
        if hasattr(st, "secrets") and st.secrets.get("GEMINI_API_KEY"):
            return st.secrets.get("GEMINI_API_KEY", "") or ""
    except Exception:
        pass
    return ""


def get_ollama_model() -> str:
    """
    å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚
    st.session_state['ollama_model'] ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ãƒ™ãƒ¼ã‚¹ã® OLLAMA_MODEL ã‚’è¿”ã™ã€‚
    """
    model = st.session_state.get("ollama_model", "").strip() if "ollama_model" in st.session_state else ""
    return model or OLLAMA_MODEL


def _ollama_chat_http(model: str, messages: list, timeout_seconds: int):
    """Ollama ã® HTTP API ã‚’ç›´æ¥å©ãã€‚requests ã® timeout ã§ç¢ºå®Ÿã«åˆ‡ã‚‹ã€‚"""
    try:
        import requests
    except ImportError:
        raise RuntimeError("requests ãŒå¿…è¦ã§ã™: pip install requests")

    base = os.environ.get("OLLAMA_HOST", "http://localhost:11434").rstrip("/")
    url = base + "/api/chat"
    payload = {"model": model, "messages": messages, "stream": False}
    try:
        resp = requests.post(url, json=payload, timeout=timeout_seconds)
    except requests.exceptions.ConnectTimeout:
        raise RuntimeError(
            f"Ollama ãŒ {timeout_seconds} ç§’ä»¥å†…ã«å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "ãƒ»ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ `ollama serve` ãŒå‹•ã„ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
            "ãƒ»ãƒ¢ãƒ‡ãƒ«ãŒé‡ã„å ´åˆã¯åˆå›ã®å¿œç­”ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚è»½ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: lease-annaï¼‰ã‚’è©¦ã™ã‹ã€Gemini API ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„ã€‚"
        )
    except requests.exceptions.ConnectionError as e:
        raise RuntimeError(
            "Ollama ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "ãƒ»ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ **ollama serve** ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
            f"ãƒ»æ¥ç¶šå…ˆ: {base}\n"
            f"ãƒ»è©³ç´°: {e}"
        )
    except requests.exceptions.Timeout:
        raise RuntimeError(
            f"Ollama ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout_seconds}ç§’ï¼‰ã€‚\n"
            "ãƒ»è»½ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆlease-anna ç­‰ï¼‰ã‚’è©¦ã™ã‹ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ Gemini API ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„ã€‚"
        )

    if resp.status_code == 404:
        try:
            err_body = resp.json()
            err_msg = err_body.get("error", resp.text)
        except Exception:
            err_msg = resp.text
        raise RuntimeError(
            f"ãƒ¢ãƒ‡ãƒ«ã€Œ{model}ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
            f"ãƒ»ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ **ollama pull {model}** ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚\n"
            f"ãƒ»ã¾ãŸã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€ŒAIãƒ¢ãƒ‡ãƒ«è¨­å®šã€ã§åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: lease-annaï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n"
            f"ãƒ»Ollamaã®è©³ç´°: {err_msg[:200]}"
        )
    resp.raise_for_status()
    data = resp.json()
    if "message" in data and "content" in data["message"]:
        return {"message": {"content": data["message"]["content"]}}
    raise RuntimeError("Ollama ã®å¿œç­”å½¢å¼ãŒä¸æ­£ã§ã™ã€‚")


def _gemini_chat(api_key: str, model: str, messages: list, timeout_seconds: int):
    """
    Gemini API ã§ãƒãƒ£ãƒƒãƒˆã€‚messages ã¯ [{"role":"user","content":"..."}] å½¢å¼ã€‚
    æœ€å¾Œã® user ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦é€ã‚Šã€è¿”ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    if not api_key or not api_key.strip():
        return {"message": {"content": "Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã¾ãŸã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"}}
    prompt = ""
    for m in messages:
        if m.get("role") == "user" and m.get("content"):
            prompt = m["content"]
    if not prompt:
        return {"message": {"content": "é€ä¿¡ã™ã‚‹å†…å®¹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"}}
    try:
        import google.generativeai as genai
    except ImportError:
        return {"message": {"content": "Gemini ã‚’ä½¿ã†ã«ã¯ pip install google-generativeai ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"}}

    try:
        genai.configure(api_key=api_key.strip())
        gemini_model = genai.GenerativeModel(model)
        try:
            config = genai.types.GenerationConfig(max_output_tokens=2048, temperature=0.7)
            response = gemini_model.generate_content(prompt, generation_config=config)
        except (AttributeError, TypeError):
            response = gemini_model.generate_content(prompt)

        if not response:
            return {"message": {"content": "Gemini ã‹ã‚‰å¿œç­”ãŒè¿”ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"}}

        text = None
        try:
            if response.text:
                text = response.text
        except (ValueError, AttributeError):
            pass
        if not text and getattr(response, "candidates", None):
            for c in response.candidates:
                if getattr(c, "content", None) and getattr(c.content, "parts", None):
                    for p in c.content.parts:
                        if getattr(p, "text", None):
                            text = (text or "") + p.text
                    if text:
                        break
        if text and text.strip():
            return {"message": {"content": text.strip()}}
        return {"message": {"content": "Gemini ã‹ã‚‰ç©ºã®å¿œç­”ã‹ã€å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰ãˆã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"}}
    except Exception as e:
        err = str(e).strip().lower()
        if "429" in err or "quota" in err or "resource_exhausted" in err or "rate limit" in err:
            return {"message": {"content": (
                "**Gemini ã®åˆ©ç”¨æ ï¼ˆç„¡æ–™æ ã®1æ—¥åˆ¶é™ï¼‰ã«é”ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**\n\n"
                "ãƒ»ç„¡æ–™æ ã¯1æ—¥ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã«ä¸Šé™ãŒã‚ã‚Šã¾ã™ã€‚\n"
                "ãƒ»æ˜æ—¥ã«ãªã‚‹ã¾ã§ãŠå¾…ã¡ã„ãŸã ãã‹ã€[Google AI Studio](https://aistudio.google.com/) ã§åˆ©ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
                "ãƒ»æœ‰æ–™ãƒ—ãƒ©ãƒ³ã«ã™ã‚‹ã¨åˆ¶é™ãŒç·©å’Œã•ã‚Œã¾ã™ã€‚\n\n"
                f"ã€APIã®è©³ç´°ã€‘{str(e)[:300]}"
            )}}
        return {"message": {"content": f"Gemini API ã‚¨ãƒ©ãƒ¼: {str(e)}\n\nAPIã‚­ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«åï¼ˆ{model}ï¼‰ã‚’ç¢ºèªã—ã€ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"}}


def _chat_for_thread(engine: str, model: str, messages: list, timeout_seconds: int, api_key: str = "", gemini_model: str = ""):
    """
    ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å‘¼ã¶ç”¨ã€‚st.session_state ã‚’å‚ç…§ã—ãªã„ã€‚
    engine ãŒ "gemini" ã®ã¨ãã¯ api_key ã¨ gemini_model ã‚’ä½¿ç”¨ã€‚
    """
    if engine == "gemini":
        api_key = (api_key or "").strip() or os.environ.get("GEMINI_API_KEY", "")
        if not api_key:
            return {"message": {"content": "Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã¾ãŸã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"}}
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
                future = ex.submit(_gemini_chat, api_key, gemini_model or "gemini-2.0-flash", messages, timeout_seconds)
                return future.result(timeout=min(timeout_seconds + 30, 90))
        except Exception as e:
            return {"message": {"content": f"Gemini ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\n\nã€è©³ç´°ã€‘{str(e)}"}}
    try:
        return _ollama_chat_http(model, messages, timeout_seconds)
    except Exception as e:
        return {"message": {"content": f"AIã‚µãƒ¼ãƒãƒ¼ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸ: {e}"}}


def chat_with_retry(model, messages, retries=2, timeout_seconds=120):
    """AI ã¸ã®ãƒãƒ£ãƒƒãƒˆå‘¼ã³å‡ºã—ã€‚ã‚¨ãƒ³ã‚¸ãƒ³ãŒ Gemini ã®å ´åˆã¯ Gemini APIã€å¦åˆ™ Ollamaã€‚"""
    engine = st.session_state.get("ai_engine", "ollama")
    if engine == "gemini":
        api_key = (st.session_state.get("gemini_api_key") or "").strip() or GEMINI_API_KEY_ENV
        api_key = api_key or _get_gemini_key_from_secrets()
        gemini_model = st.session_state.get("gemini_model", GEMINI_MODEL_DEFAULT)
        if "last_gemini_debug" not in st.session_state:
            st.session_state["last_gemini_debug"] = ""
        for i in range(retries):
            try:
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
                    future = ex.submit(_gemini_chat, api_key, gemini_model, messages, timeout_seconds)
                    try:
                        out = future.result(timeout=min(timeout_seconds + 30, 90))
                    except concurrent.futures.TimeoutError:
                        st.session_state["last_gemini_debug"] = "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆå¿œç­”ãŒè¿”ã‚‹ã¾ã§å¾…ã¡ã¾ã—ãŸãŒå¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"
                        st.error("Gemini ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã™ã‚‹ã‹ã€ã—ã°ã‚‰ãã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
                        return {"message": {"content": "Gemini ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã™ã‚‹ã‹ã€ã—ã°ã‚‰ãã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"}}
                content = (out.get("message") or {}).get("content", "")
                st.session_state["last_gemini_debug"] = "OK" if content and "APIã‚­ãƒ¼ãŒ" not in content and "Gemini API ã‚¨ãƒ©ãƒ¼:" not in content else (content[:200] + "..." if len(content or "") > 200 else (content or "ï¼ˆç©ºï¼‰"))
                if content and (
                    "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“" in content
                    or "Gemini API ã‚¨ãƒ©ãƒ¼:" in content
                    or "pip install" in content
                    or "å¿œç­”ãŒè¿”ã‚Šã¾ã›ã‚“ã§ã—ãŸ" in content
                    or "å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§ãƒ–ãƒ­ãƒƒã‚¯" in content
                    or "åˆ©ç”¨æ " in content
                    or "ç„¡æ–™æ " in content
                ):
                    st.error(content)
                return out
            except Exception as e:
                err = str(e)
                st.session_state["last_gemini_debug"] = f"ä¾‹å¤–: {err}"
                if "429" in err or "quota" in err.lower() or "resource_exhausted" in err.lower() or "rate limit" in err.lower():
                    time.sleep(2 * (i + 1))
                    continue
                st.error(f"Gemini API ã‚¨ãƒ©ãƒ¼: {err}")
                return {"message": {"content": f"Gemini ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\n\nã€è©³ç´°ã€‘{err}"}}
        st.session_state["last_gemini_debug"] = "ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ï¼ˆã¾ãŸã¯åˆ©ç”¨æ ã®å¯èƒ½æ€§ï¼‰"
        return {"message": {"content": (
            "Gemini ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"
            "**ç„¡æ–™æ ã®1æ—¥ã‚ãŸã‚Šã®åˆ¶é™ã«é”ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**\n"
            "ãƒ»æ˜æ—¥ã¾ã§ãŠå¾…ã¡ã„ãŸã ãã‹ã€[Google AI Studio](https://aistudio.google.com/) ã§åˆ©ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
            "ãƒ»APIã‚­ãƒ¼ãƒ»ãƒ¢ãƒ‡ãƒ«åãƒ»ãƒãƒƒãƒˆæ¥ç¶šã‚‚ã‚ã‚ã›ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )}}

    last_error = None
    for i in range(retries):
        try:
            return _ollama_chat_http(model, messages, timeout_seconds)
        except Exception as e:
            last_error = str(e)
            if "429" in last_error:
                time.sleep(2 * (i + 1))
                continue
            break

    if last_error:
        st.error(f"AIã‚µãƒ¼ãƒãƒ¼ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸ: {last_error}")
        detail = f"\n\nã€æŠ€è¡“çš„ãªè©³ç´°ã€‘{last_error}"
        if "timed out" in last_error or "Timeout" in last_error:
            detail += "\n\nğŸ’¡ å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€ŒAIãƒ¢ãƒ‡ãƒ«è¨­å®šã€ã§ **Gemini API** ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã‹ã€**lease-anna** ç­‰ã®è»½ã„ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚"
    else:
        st.error("AIã‚µãƒ¼ãƒãƒ¼ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
        detail = ""
    return {
        "message": {
            "content": "AIãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æ™‚é–“ã‚’ç½®ãã‹ã€Gemini API ã«åˆ‡ã‚Šæ›¿ãˆã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚" + detail
        }
    }


def generate_battle_special_move(strength_tags: list, passion_text: str) -> tuple:
    """
    å®šæ€§ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Œå¿…æ®ºæŠ€åã€ã¨ã€Œç‰¹æ®ŠåŠ¹æœã€ã‚’1ã¤ç”Ÿæˆã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: (name: str, effect: str)ã€‚å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿”ã™ã€‚
    """
    fallback = ("é€†è»¢ã®æ„æ°—", "ã‚¹ã‚³ã‚¢+5%")
    if not strength_tags and not (passion_text or "").strip():
        return fallback
    model = get_ollama_model() if st.session_state.get("ai_engine") == "ollama" else GEMINI_MODEL_DEFAULT
    tags_str = "ã€".join(strength_tags) if strength_tags else "ãªã—"
    text_snippet = (passion_text or "")[:300]
    prompt = f"""ä»¥ä¸‹ã‹ã‚‰ã€å¯©æŸ»ã‚²ãƒ¼ãƒ ç”¨ã®ã€Œå¿…æ®ºæŠ€ã€ã‚’1ã¤ã ã‘è€ƒãˆã¦ãã ã•ã„ã€‚
å¼·ã¿ã‚¿ã‚°: {tags_str}
ç†±æ„ãƒ»è£äº‹æƒ…ï¼ˆæŠœç²‹ï¼‰: {text_snippet or "ãªã—"}

å¿…æ®ºæŠ€ã¯ã€Œåå‰ã€ã¨ã€ŒåŠ¹æœã€ã®2ã¤ã ã‘ã€‚1è¡Œã§ç­”ãˆã¦ãã ã•ã„ã€‚å½¢å¼ã¯å¿…ãš:
å¿…æ®ºæŠ€å / åŠ¹æœã®çŸ­ã„èª¬æ˜
ä¾‹: è€èˆ—ã®æš–ç°¾ / ãƒ€ãƒ¡ãƒ¼ã‚¸ç„¡åŠ¹
ä¾‹: æ¥­ç•Œäººè„ˆã®ç›¾ / æµå‹•æ€§+10%
æ—¥æœ¬èªã§ã€å¿…æ®ºæŠ€åã¯10æ–‡å­—ä»¥å†…ã€åŠ¹æœã¯15æ–‡å­—ä»¥å†…ã€‚ä»–ã¯å‡ºåŠ›ã—ãªã„ã€‚"""
    try:
        out = chat_with_retry(model, [{"role": "user", "content": prompt}], retries=1, timeout_seconds=15)
        content = ((out.get("message") or {}).get("content") or "").strip()
        if " / " in content:
            parts = content.split(" / ", 1)
            return (parts[0].strip()[:20] or fallback[0], (parts[1].strip()[:25] or fallback[1]))
    except Exception:
        pass
    return fallback


def is_ollama_available(timeout_seconds: int = 3) -> bool:
    """
    Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã‚’ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚
    èµ·å‹•ã—ã¦ã„ãªã„çŠ¶æ…‹ã§ chat_with_retry ã‚’å‘¼ã¶ã¨æ°¸é å¾…ã¡ã«ãªã‚Šã‚„ã™ã„ã®ã§ã€
    äº‹å‰ã«ã“ã“ã§æ¤œçŸ¥ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¡ˆå†…ã‚’å‡ºã™ã€‚
    """
    try:
        import requests
    except ImportError:
        return False

    base = os.environ.get("OLLAMA_HOST", "http://localhost:11434").rstrip("/")
    url = base + "/api/tags"
    try:
        resp = requests.get(url, timeout=timeout_seconds)
        return resp.status_code == 200
    except Exception:
        return False


def is_ai_available(timeout_seconds: int = 3) -> bool:
    """
    ç¾åœ¨é¸æŠä¸­ã®AIã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã€‚
    Gemini ã®å ´åˆã¯ API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã° Trueã€‚
    Ollama ã®å ´åˆã¯ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚Œã° Trueã€‚
    """
    engine = st.session_state.get("ai_engine", "ollama")
    if engine == "gemini":
        key = st.session_state.get("gemini_api_key", "").strip() or GEMINI_API_KEY_ENV
        key = key or _get_gemini_key_from_secrets()
        return bool(key)
    return is_ollama_available(timeout_seconds)


def run_ollama_connection_test(timeout_seconds: int = 10) -> str:
    """
    Ollama ã®æ¥ç¶šã¨ãƒ¢ãƒ‡ãƒ«å¿œç­”ã‚’ãƒ†ã‚¹ãƒˆã—ã€çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã€‚
    ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒOllamaæ¥ç¶šãƒ†ã‚¹ãƒˆã€ãƒœã‚¿ãƒ³ç”¨ã€‚
    """
    try:
        import requests
    except ImportError:
        return "âŒ requests ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: pip install requests"

    base = os.environ.get("OLLAMA_HOST", "http://localhost:11434").rstrip("/")
    model = get_ollama_model() or OLLAMA_MODEL

    try:
        r = requests.get(base + "/api/tags", timeout=5)
        if r.status_code != 200:
            return f"âŒ Ollama ã‚µãƒ¼ãƒãƒ¼å¿œç­”ç•°å¸¸: {base} (HTTP {r.status_code})"
    except requests.exceptions.ConnectionError:
        return (
            f"âŒ Ollama ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚\n"
            f"æ¥ç¶šå…ˆ: {base}\n\n"
            "**å¯¾å‡¦:** ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n"
            "```\nollama serve\n```"
        )
    except requests.exceptions.Timeout:
        return f"âŒ Ollama ã‚µãƒ¼ãƒãƒ¼ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸï¼ˆ5ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰ã€‚\næ¥ç¶šå…ˆ: {base}"

    try:
        r = requests.post(
            base + "/api/chat",
            json={"model": model, "messages": [{"role": "user", "content": "ã“ã‚“"}], "stream": False},
            timeout=timeout_seconds,
        )
        if r.status_code == 404:
            return (
                f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼ã¯å‹•ã„ã¦ã„ã¾ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ã€Œ{model}ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n"
                f"**å¯¾å‡¦:** ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n"
                f"```\nollama pull {model}\n```\n\n"
                "ã¾ãŸã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: lease-annaï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
            )
        r.raise_for_status()
        data = r.json()
        content = (data.get("message") or {}).get("content", "")
        if content:
            return f"âœ… æ¥ç¶šOKï¼ˆãƒ¢ãƒ‡ãƒ«: {model}ï¼‰\nå¿œç­”: {content[:80]}{'â€¦' if len(content) > 80 else ''}"
        return f"âœ… æ¥ç¶šOKï¼ˆãƒ¢ãƒ‡ãƒ«: {model}ï¼‰\nï¼ˆå¿œç­”æœ¬æ–‡ã¯ç©ºã§ã—ãŸï¼‰"
    except requests.exceptions.Timeout:
        return (
            f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã€Œ{model}ã€ãŒ {timeout_seconds} ç§’ä»¥å†…ã«å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"
            "ãƒ»åˆå›ã¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã§æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n"
            "ãƒ»è»½ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆlease-anna ç­‰ï¼‰ã‚’è©¦ã™ã‹ã€Gemini API ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„ã€‚"
        )
    except Exception as e:
        return f"âŒ ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}"


def save_debate_log(data: dict):
    """ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆçµæœã‚’ä¿å­˜"""
    data["timestamp"] = datetime.datetime.now().isoformat()
    try:
        with open(DEBATE_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(data, ensure_ascii=False) + "\n")
    except Exception as e:
        st.error(f"ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
